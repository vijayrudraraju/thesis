\resetdatestamp

\chapter{User Interface and Data Visualization Design}

The scope of the discipline of \emph{human-computer interaction} (and even the legitimacy of the endevour) varies depending on who is asked, however at present it is understood to involve "the design, implementation and evaluation of interactive systems in the context of the user's task and work" \cite{hci1998}. Within the broad domain of interactive systems, various aspects of a system are treated by different disciplines within human-computer interaction (HCI). Designing a user interface for manipulating a Libmapper network is an interesting task because the design problem is a composite of two interrelated subproblems. The first is visualizing the current state of the network, as reported by communications with the network through the Mapper Protocol, in such a way as to allow a user to quickly reason about the current state of the network and what actions should be taken to move the network to a desired state. The second is allow the potential actions that the user might take to be executed in the interface as painlessly as possible. Conveniently there are two disciplines that task themselves with analyzing these two problems: \emph{user interface design} and \emph{data visualization design}. Literature in these fields have much to say on the topic of how to proceed in the design of an interface and this section will focus on the most relevant findings for Vizmapper beginning with a history of user interfaces in the context of the development of general purpose computers.

\section{Short History of User Interfaces}

The term \emph{user interface} is a byproduct of historical developments in computing and makes sense only in the context of current computing culture. The use of \emph{user interface} as a distinct concept was not needed when the \emph{user} was certain to be the engineer or programmer involved in the creation of the program and/or the machine that was to run the program; at the very least, they were comfortable interacting with the software in the same mode as the original programmer and did not require an explicit interface to augment the usability of the system \cite{continuity1990}. The term \emph{computer interface} might more precisely communicate the aspect of the system that is actually being addressed by the term \emph{user interface}, namely the computer's interface to the world - whether the \emph{user} is assumed to be a programmer, a musician, another machine, the larger environment, or a poodle. 

The concerns of user interface designers have changed considerably since people first started interacting with programmable digital computers for processing, storing, and retrieving information. Understanding the context of and nature of the interdependant effect that fundemental developments in computing and their accompanying user interfaces had on each other is essential for appreciating the meaningful impact that a seemingly frivolous thing like user interface can have on the development of the basic functionality of the computer - for example, using a computer for creating mappings between a distributed network of devices.  

\subsection{Punch cards}

The first interfaces largely depended on punch cards and paper tape as control mechanisms and paper printers for output and program feedback. On machines used during the 1950s and 1960s, in order to input a program and a dataset into a computer, first one prepares a deck of punched cards using a typewriter-like machine, then one feeds the deck to the computer \cite{oshistory2011}. Optionally on some machines, one can mount reels of magnetic tape that store oft-used or previously generated datasets or precompiled complimentary software. On these early machines, a particular program being run from a deck of punch cards would be given control of the resources of the entire computer. There would be no other resident software running on the machine handling auxilery tasks. Any hardware devices required by a particular program like punch card readers and line printers had to be handled completely by the user program contained in the deck of cards fed to the machine. 

\emph{Operating systems} development grew out of the growing complexity of computer systems involving many cooperating devices and the mirrored growth of the complexity of the programs that were written for them. It makes little sense for a user of a machine to write basic operating software for interfacing with the same perhipheral devices everytime they want to run a program on such a machine. Especially since any user of a computer during the 1960s likely needed to enter a queue to reserve time to run their program on a single, widely shared machine and any time wasted debugging low-level interactions with input and output devices wasted valuable machine time. Consequently, machines begain appearing with libraries of code that took care of low-level control and provided easier access to input and output devices as always resident services. This collection of software that is permenantly resident on a machine became known as a \emph{batch monitor} \cite{os2000}. A user's program could link to these preloaded libraries without including the operating logic explicitly within their own program code. This shift is often cited as the genesis of the modern operating system.

In addition to providing access to input/output devices, the monitor often provided services to perform error checking on user submitted programs and for generating useful feedback to the user concerning the progress of execution of a user program. The idea of generating "useful" feedback, and what that might mean, can be understood as the first instance of an explicitly designed user interface and the birth of user interface design \cite{unix2008}.

\subsection{Command-line}

Command-line interfaces are the next step in the evolution of computer interfaces and the link between batch systems of the 1950s and what would be recognized as a \emph{graphical user interface} (GUI) today. With continual, steady reduction of the amount of time needed to complete a computation cycle as processors technology advanced, it became possible to interact with the computer with a series of requests and on demand execute programs and receive responses expressed as specially formatted strings of text using a specialized vocabulary. Requests could be completed in seconds, no longer hours and days, and it made sense for the user to simply wait for the request to be completed before entering in the next request. This is as opposed to queueing up a mult-stage program with a stack of requests and waiting for all the requests to be completed or for one of the requests to fail. Instead, the user can change their mind about the structure of later stages in the program in response to feedback from earlier stages. This introduces the possibility for software to explore a set of possibilities with the guidance of the user and allows for a type of interactivity not possible with the batch systems of the 1950s. 

Although the earliest command-line systems borrowed typewriter-like teletypes (as used for telegraph transmission) as the input and output mechanism, by the 1970s video display terminals were used with computers for providing text feedback on a virtual canvas of pixels that could be rapidly and reversibly modified (unlike teletype printers) and it became possible for a program to display an interface that could be called visual as well as just textual \cite{unix2008}. This allowed programmers to create the first computer games and text editors that relied on this capability of video display terminals.

\subsection{Graphical}

Further reduction of the amount of time needed to execute individual operations within a program resulted in the ability for the computer to communicate with multiple input/output devices in realtime. Typical devices that a modern computer program expects to have access to through the operating system include a color monitor wherein each pixel in the monitor had a separate referenceable address, a graphics card to help with processing of 2d/3d graphics operations, a mouse, a keyboard, and a sound card connected to audio speakers.

Much of the common grammar of all the popular graphical user interfaces in use today were developed by two particular projects. The first is called NLS/Augment (NLS stands for oN-Line System) and was designed by Douglas Engelbart and his team at the Stanford Research Institute. During a famous, public demonstration of the system in 1968 (affectionately, often referred to as "The Mother of All Demos"), Engelbart proceeded to demonstrate the use of a computer mouse, a graphical display with multiple windows, and hyperlinks among many other notable advancements in human-computer interaction. 

The second groundbreaking project, the Xerox Alto, came from the Xerox Palo Alto Research Center (PARC) in 1973. It is perhaps the first computer designed from inception to be dedicated to the use of a single person. Although the monitor displayed only black and white pixels, the graphical user interface of the operating system contained buttons, windows, scrollbars, sliders, and many of the logical GUI components that are standard components of any program with a GUI.

It is primarily the ideas that these two projects consolidated and introduced to the wider world that eventually became the impetus for the recognition of HCI and, of more precise relevance to this paper, user interface design as important areas of study of significant relevance to the broader computer science community. 

\subsection{Five Foci of Interface Development}

It is clear from history that, in some sense, computing machines have always had user interfaces. What has changed is the the level of attention that any particular layer in the computing interface has received during various periods of history. Grudin \cite{continuity1990} introduces a helpful framework for understanding this change that took place during the second half of the 20th century as movement through five different foci of interface design. 

Initially, during and before the days of punch cards, most users of computers were electrical engineers primarily concerned with working directly with the hardware (flipping switches, replacing vacuum tubes, wiring ports and circuits). Conceptually, the computer interface was located at the hardware itself. 

Afterwards, the primitive operating systems and batch monitors placed the majority of focus on the task of programming and the environment in which commands were executed and requests were made and began shifting focus away from the hardware. The development of ever higher-level languages and environments eventually replaced the need to be familiar with hardware particulars. At this point, the primary interface in these systems was at the level of the software code and programming. 

As machines began to perform their operations more quickly, systems became interactive and users were not expected to be able to interact with the computer through computer code, the interface grew into the canonical monitor, keyboard, and mouse of the 1990s and 2000s. It made sense for designers of computer systems to become more concerned with issues of perception and motor control. 

In recent years it has become practical to interact with the computer in a more conversational/realtime manner and system designers focus on the entire process of a user interacting with the system over an extended period of time, from initial exposure to practiced expert interaction. The focus of the interface designer becomes less trained on issues of ergonomics and visual perception and more trained on deeper cognitive issues like learning and problem solving. The interface has grown to encompass the proclivities of the mind. 

Impressively, Grudin forsaw the advent of social networking in what he labels \emph{groupware}, the fifth focus of interface development. When a single system serves a large group of interacting people as opposed to an individual, the user interface becomes the dynamics of interaction between the members of the group.

Ultimately, Grudin cites these observations within the context of the framwork of five foci to support the assertion that the history of the computer has been one of the computer extending its interface with the world outward from its hardware implementation and deep into the intanglible aspects of human behavior and a broader understanding of how we accomplish tasks in everyday life \cite{continuity1990}.

\section{Task Analysis}

Task analysis is a class of techniques used in HCI to guide the design of user interfaces. One of the central assumptions of user interface design and HCI is that different tasks require different interfaces. A corollary of this premise is that thinking carefully about the nature of the tasks that are meant to be accomplished by the interface will lead to better design decisions. Crystal and Ellington (2004) performed an in-depth comparative analysis of the dominant techniques and provide a good introduction to the motivations of task analysis:

\begin{quote}
Practitioners and researchers routinely advocate building user-centered systems which enable people to reach their goals, take account of natural human limitations, and generally are intuitive, efficient and pleasurable to use (Preece, Rogers and Sharp, 2002). Central to the design of such systems is a clear understanding of what users actually want to do: What are their tasks? What is the nature of those tasks? Many techniques have been proposed to help answer these questions. Task analysis techniques are particularly important because they enable rigorous, structured characterizations of user activity. They provide a framework for the investigation of existing practices to facilitate the design of complex systems.

Task analysis is especially valuable in the context of human-computer interaction (HCI). User interfaces must be specified at an extremely low level (e.g. in terms of particular interaction styles and widgets), while still mapping effectively to users high-level tasks. Computer interfaces are often highly inflexible (when compared to interacting with a physical environment or another person). This inflexibility magnifies the impact of interface design problems, making the close integration of task structure and interface support especially crucial. \cite{crystal2004}
\end{quote}

Broadly, task analysis consists of the observational and heuristic analysis of the physical, mental, and contextual requirements for performing a specific task. As such, even in its most rigorous and quantitative forms, task analysis typically involves less quantitative methods like discourse analysis, contextual inquiry, and video analysis. 

The roots of scientific task analysis go back to 1911 when Frederick Taylor published \emph{The Principles of Scientific Management} \cite{crystal2004}. Taylor was interested in improving manufacturing productivity and incorporating understanding of human factors into work methods. Known commonly as Taylorism, he argued that managers should rigorously systematize the organization of workers based on empirical evidence. Of course, it is more accurate to refer to Taylor's discipline as something like job design; however, the relevant point here is that effort was being made to examine the efficacy of performing a task in one way as opposed to another. The psychological component of such job design was first examined by Harvard Business School between 1927 and 1932 at the Western Electric Hawthorne Plant. The studies essentially concluded that the psychology of individuals with the workplace contribute significantly to what workers produce and expect from their jobs.

It soon became commonplace for industrial engineers to incorporate analyses of production methods to improve interaction between humans and machines. As computers became a common machine that humans were interacting with in the workplace and the power and flexibility of computers as tools has expanded, human-computer interaction (HCI) morphed into a dedicated discipline. 

This increased flexibility has meant that computers are becoming entangled in new areas of human behavior like music. Multiple techniques have developed to deal with this greater scope and complexity required of task analysis and each technique focuses on different aspects and contributes different insights into the nature of a human task.

An important point that should be extracted from this history, regarding the use of HCI techniques like task analysis to analyse artistic systems, is that HCI and thus user interface and data visualization design, has borne the fingerprints of a discipline that was ultimately derived from the standpoint of increasing productivity and improving job performance. Therefore, the principles advanced by HCI should always be evaluated (and potentially ignored) in the broader context of particular scenarios. 

\section{Graphical Perception and Information Seeking}

\begin{quote}
Graphics is the visual means of resolving logical problems. \cite{bertin1981}
\end{quote}

This idea from Bertin, who conducted one of the first attempts to provide a theoretical foundation to information visualization, summarizes the hope for a dedicated interface for configuring a Libmapper network. It is in this context that \emph{data visualization} or \emph{information visualization} is understood to be "the computer-assisted use of visual processing to gain understanding" by Card \cite{card1997}. 

The task of mapping is a logical problem, as well as an artistic one. It is a logical problem in the sense that one cannot make connections haphazardly between any pair of input and output signals and expect the network mapping that is produced to be interesting as a DMI or meet the goals of the design team. One must still infer, through some form of reasoning, the suitability of a particular signal connection in the context of the particular devices on the network, the nature of the signals that the devices produce, and the broader artistic intentions of the design team. 

It could be that one output signal generates floating-point decimal values and one input signal accepts only integer values. Or if a specific output signal tends to generate an essentially static signal regardless of the gestures performed with the MSN, it would make for a very dull and non-dynamic performance to connect this signal to the pitch input signal of a audio synthesizer. Of course, as previously stated one does not typically make use of metrics like productivity or performance in an artistic context, so the logical problem is different. But even an artistic project has goals and intentions, though they are considerably harder to define in words. Visualizing the Libmapper network effectively within a user interface will likely help a team of people (as long as there is agreement about the artistic and other intentions of the scenario that the MSN is to be used in), resolve these logical problems involving network topology and signal transformation more effectively. 

According to Bertin, graphics have at least two distinct uses. The first is as a means of communicating some information. The second is as a medium for graphical processing, defined as using the manipulation and perception of graphical objects to understand the information \cite{card1997}. Any graphical interface that is monitoring a Libmapper network will likely incorporate both uses of graphics. The current state of the network, including the namespaces and properties of all registered devices and signals and all signal connections and transformations, needs to be able to be gleaned from the interface. Also, the interface must allow the user to manipulate the graphics in such a way as to facilitate the connection and disconnection of signals with or without functional transformations applied to the signals.

It is useful to distinguish between three types of data as distinguished by their function \cite{card1997}:
\begin{description}
\item \emph{Nominal} - data values that are understood as either = or not = to each other - an example is the type of things 
\item \emph{Ordinal} - data values that can be understood using a greater-than or less-than relation in some way - an example is set of the days of the week
\item \emph{Quantitative} - data values that can be manipulated with arithmetic - an example is the population of a city 
\end{description}

\begin{table}
    \begin{center}
    \begin{tabular}{ | l | l | l | }
    \hline
    Quantitative & Ordinal & Nominal \\ \hline
    Position & Position & Position \\
    Length & Density & Color Hue \\
    Angle & Color Saturation & Texture \\
    Slope & Color Hue & Connection \\
    Area & Texture & Containment \\
    Volume & Connection & Density \\
    Density & Containment & Color Saturation \\
    Color Saturation & Length & Shape \\
    Color Hue & Angle & Length \\
    \cellcolor{black}\textcolor{white}{Texture} & Slope & Angle \\
    \cellcolor{black}\textcolor{white}{Connection} & Area & Slope \\
    \cellcolor{black}\textcolor{white}{Containment} & Volume & Area \\
    \cellcolor{black}\textcolor{white}{Shape} & \cellcolor{black}\textcolor{white}{Shape} & Volume \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Ranking of perceptual tasks for each fundemental data type}
    \label{tab:perceptual}
\end{table}

Depending on the functional type of the data being visualized, certain forms of visual differentiation will be more effective than others. Card et al. \cite{card1997} uses the distinction between \emph{automatic processing} and \emph{controlled processing} capacities in human visual processing to inform an analysis of visualization techniques. Automatic processing works on visual properties like position and color and is highly parallelized, but lacks power. Controlled processing which works, notably, on text and is very powerful, but is not very parallelized because it requires conscious attention \cite{controlauto1977}. 

In this language, any visualization is composed from \emph{marks} (points, lines, areas, surfaces, or volumes), \emph{automatically processed graphical properties} of those marks (position, color, size, shape, etc...), and \emph{controlled processing graphical properties} like alphabet characters. An oft-cited study by Cleveland and McGill \cite{cleveland1984} forms the basis of a theory about the accuracy with which people infer quantitative information from the various perceptual tasks that could be employed in a graphical presentation of data. This theory is expanded upon by Mackinlay \cite{jock1986} to form the ranking displayed in Table \ref{tab:perceptual}. This theory is useful for deciding how to map graphical properties to various types of information because it characterizes the effectiveness of various graphical languages. Mackinlay motivates the importance of the particular ranking by introducing the common sense \emph{Principle of Importance Ordering}:

\begin{quote}
Encode more important information more effectively. \cite{jock1986}
\end{quote}

\begin{comment}
The Structure of the Information Visualization Design Space, Section 2 
controlled vs. automatic processing
connection

Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays
\end{comment}

This theoretical framework will be used in the next major section to weigh various visualization choices within Vizmapper.

\section{Recall and Recognition}

It is common to understand human information retrieval in terms of two types of retrieval, recall and recognition. In the recall process, information is reproduced from memory. In the recognition process, a piece of information induces the knowledge that the information is not new; it has been seen before and the information acts as a cue. In this respect, recognition is a less complex cognitive activity.

Recall can be assisted with cues that allow an individual to speed access to the information in their memory. One proven cue is the use of categories.

\section{Hierarchies and Taxonomies}

\begin{comment}
Magic number seven, George Miller
recoding pg. 93
increasing bits per chunk
The Structure of the Information Visualization Design Space, Section 2
enclosure
\end{comment}

\begin{quote}
In the jargon of communication theory, this process would be called \emph{recoding}. The input is given in a code that contains many chunks with few bits per chunk. The input is given in a code that contains many chunks with few bits per chunk. The operator recodes the input into another code that contains fewer chunks with more bits per chunk. There are many ways to do this recoding, but probably the simplest is to group the input events, apply a new name to the group, and then remember the new name rather than the original input events. \cite{seven1956}
\end{quote}

\section{Modal Interfaces}

\section{Filtering}

\begin{comment}
The Structure of the Information Visualization Design Space, Section 5
dynamic queries technique 

Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays
\end{comment}
